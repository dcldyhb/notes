---
title: 人工智能导论
category:
  - 本科课程
  - 编程
tags:
  - 专业课程
  - 编程
  - 人工智能
math: true
date: 2025-9-18 13:17
---

# 人工智能导论

## 导论

### 什么是人工智能

关于什么是人工智能没有一个统一的定义

#### 人工智能的分类

| 类型            | 功能                                        |
| --------------- | ------------------------------------------- |
| 狭窄 AI         | 以目标为导向的 AI，经过训练后可执行特定任务 |
| 通用 AI (？)    | 能够像人类一样独立思考的系统                |
| 超级 AI (!)     | 超越人类智能，可以比人类更好地完成任务      |
| 反应型机器      | 专注于当前场景，并根据最佳行动做出反应      |
| 有限记忆机器    | 可以在短时间内存储和使用过去的经验或数据    |
| 理论型 AI (!)   | 能够理解人类情感并像人类一样进行社交互动    |
| 自我意识 AI (!) | 拥有自我意识和自我理解，能够独立思考和决策  |

### 人工智能发展历程

#### 图灵测试 (1950)

- **图灵的逻辑是：** 如果一台机器在行为表现上和人类无法区分，那么我们就没有理由说它“不能思考”。
- **模仿游戏 (The Imitation Game) ：**
  - 测试者 (C) 在一个房间，通过打字终端与另外两个房间的“对象”交流。
  - 一个是人类 (B) ，一个是机器 (A) 。
  - 如果测试者 (C) 在经过一段时间的提问后，无法分辨出哪个是人、哪个是机器，或者误判率很高，那么这台机器就通过了图灵测试。

它只关心机器能不能**表现得像人一样**

图灵测试只能证明机器具有**语法处理能力 (Syntax) **，无法证明机器具有**语义理解能力 (Semantics) **。机器可能只是在“装懂”，而不是“真懂”

- **已经不再是最高标准：** 现在的 GPT-4 在很多对话中已经能轻易骗过人类。AI 现在的目标不再是“像人” (因为人类有认知局限) ，而是“超越人” (成为更有用的工具) 。
- **新的测试标准：** 现在的 AI 评估更多使用 GLUE、MMLU 等基准测试，考察逻辑推理、多模态理解、代码生成等硬核能力，而不是看它会不会像人一样闲聊。
- **图灵的预言：** 图灵当年预测 2000 年左右会有机器通过测试。虽然时间晚了一些，但他关于“智能应该从行为来定义”的思想，依然是现代行为主义 AI 的基石

#### 人工智能概念的诞生 (1956)

1956 年，达特茅斯学院的数学教授约翰·麦卡锡组织了一次达特茅斯夏季人工智能研究项目(DSRPAI)。“人工智能”这一术语就是在这次活动中创造的 (第一次正式提出人工智能的概念)

达特茅斯会议被认为是人工智能概念的发源地

#### 感知机的发明 (1957)

1957 年，弗兰克·罗森布拉特制造了 Mark I 感知器。该模型根据人类神经元的生物学原理构建的模拟神经网络，并展现了学习能力

感知器可以通过反复试验来学习新技能，为==现代神经网络==奠定了基础。

#### 人工智能第一个寒冬 (1970s)

最大的障碍是缺乏计算能力，无法完成实质性任务：计算机==无法存储足够的信息==，也无法足够快地处理信息。

很多报告开始批评人工智能研究缺乏进展，即使是最先进的程序也只能处理简单的问题，人工智能研究经费被削减，发展停滞不前。人工智能引来了历史上的第一个寒冬

#### 专家系统的兴起 (1980s)

80 年代初，爱德华·费根鲍姆引入了专家系统，模仿人类专家的决策过程

该程序会询问某个领域的专家如何在特定情况下做出反应，一旦学会了几乎所有情况，非专家就可以从该程序中获得建议。

#### 人工智能第二个寒冬 (1987-1993)

专家系统的维护成本比较高。专家系统也很难更新，也无法“学习” ，专家系统市场开始下滑并最终在 1987 年崩溃

#### 深蓝击败卡斯帕罗夫 (1997)

1997 年，IBM 的“深蓝”国际象棋计算机在国际象棋比赛中击败了现任世界冠军加里·卡斯帕罗夫，这提升了公众对人工智能的认知。这场比赛是现场直播的

#### 深度学习的应用

2006 年, 杰弗里·辛顿在一篇文章中提出了“深度置信网络”

2012 年，Alex 在 ImageNet 竞赛中使用深度卷积神经网络取得了显著的成果。==认为是深度学习的开端==, 其内核是卷积神经网络

#### AlphaGo 击败李世石 (2016)

#### 下一步?

未来人工智能可能的发展 (思考题)

1. 从 AI 大模型迈向通用人工智能
2. 合成数据打破人工智能训练数据瓶颈
3. 量子计算机可能率先应用于人工智能
4. 在低容错场景下 AI 的可靠、可解释性
5. AI 所带来的伦理问题需要重视

### 重要概念

#### 神经网络

一种模仿人类大脑特征的计算模型。它通过模拟生物神经元的工作原理，让计算机能够进行复杂的逻辑推理和决策

![神经网络](https://raw.githubusercontent.com/dcldyhb/Freshman-Notes-Image-Host/main/20260115222651.png)

这个也被称为全连接神经网络, 是最经典的神经网络

#### 机器学习和深度学习

![机器学习和深度学习](https://raw.githubusercontent.com/dcldyhb/Freshman-Notes-Image-Host/main/202601152230891.png)

- 机器学习是现在认为实现人工智能最重要的途径
- 深度学习是机器学习的一个分支,强调的是用比较深层的神经网络来构建这个模型

##### 机器学习的类别

1. 监督学习
   - 给带标签的数据
   - 得到一个映射关系
   - 用于识别、回归等
2. 非监督学习
   - 给不带标签的数据
   - 找到数据的内在结构
   - 用于聚类、降维等
     提取关键信息,用于数据压缩或者为后续监督学习做准备
3. 强化学习
   - 在实战中通过奖励和惩罚实现
   - 智能体通过与环境的互动来学习
     1. 智能体观察获取当前状态
     2. 智能体采取动作
     3. 环境根据动作反馈奖励或惩罚, 并且更新状态
   - 学习从状态到动作的映射

## 传统人工智能技术

### 知识表示

#### ==逻辑表示==

分为两种类型

1. 命题逻辑

   - 以 Bool 值表示对象

     $$
     \text{P}\rightarrow\text{Q}
     $$

     (如果 P 为真则 Q 为真)

2. 一阶逻辑

   - 使用量词和谓词来表示对象

     $$
     \forall x(\text{Cat}(x)\rightarrow \text{Meows}(x))
     $$

     所有猫都会叫

#### 语义网络表示

使用谓词逻辑的代替方案,使用==图结构==表示知识

节点为概念, 边表示概念之间的语义关系

![语义网络](https://raw.githubusercontent.com/dcldyhb/Freshman-Notes-Image-Host/main/202601152259410.png)

节点表示概念，边表示概念之间的语义关系。

#### 框架表示

结构化的知识表示方式

每个框架由多个槽和槽值组成

![框架表示](https://raw.githubusercontent.com/dcldyhb/Freshman-Notes-Image-Host/main/202601152315573.png)

### 推理

#### 演绎推理

- 从一般的规则出发推导出特定的结论
- 只要前提为真那么结论必然为真

#### 归纳推理

- 从具体的实例中总结出一般的规则
- 结论不一定为真，具有概率性

### 专家系统

专家系统根据用户要求，使用推理规则从知识库中提取知识，进而提供像人类专家一样的决策来解决复杂问题

==三个组成部分==

- 知识库
  - 存储从特定领域的不同专家处获得的知识的数据库
- 推理系统
  - 系统的主要处理单元。它将推理规则应用于知识库以得出结论或推断新信息
- 用户界面
  - 用户将查询以可读格式作为输入，并将其传递给推理系统。在从推理引擎获得响应后，将输出显示给用户

### 搜索策略

#### 无信息搜索

以暴力搜索的方式运行，也称为盲目搜索，不具备关于状态或搜索空间的额外信息，仅依赖于遍历树结构

- 广度优先搜索（BFS）
  - 逐层遍历，使用先进先出队列，先遍历离根节点近的节点
- 深度优先搜索（DFS）
  - 逐路径遍历，使用后进先出栈，先遍历离根节点远的节点
  - 或者使用递归实现（隐式栈）
- 深度限制搜索
  - 通过仅执行到预定义的级别的 DFS 来限制搜索的深度
- 均匀代价搜索（UCS）
  - 定义成本函数 $f(n)$，给出了从初始步骤到每个节点 $n$ 的路径的成本
  - 在探索所有可能的路径后，选择成本最低的路径

#### 启发式搜索

利用问题领域的知识（启发式信息）来引导搜索方向，从而寻找最优路径

可能并不总是提供最佳解决方案，但它保证在相当长的时间内找到一个好的解决方案

启发式函数确定状态与期望结果的接近程度，计算两个状态之间理想路径的成本，用 $h(n)$ 表示

- 最佳优先搜索
  - 利用了启发式函数 $h(n)$，选择当前看起来最有吸引力的路径，即 $h(n)$ 最小
- A\* 搜索
  - 结合了实际代价 $g(n)$ 和估计代价 $h(n)$，即 $f(n)=g(n)+h(n)$ 是常见搜索算法中最优秀的，能解决困难问题，但需要大量内存

#### 局部搜索

不从根节点开始搜索整个路径，而是从一个初始解出发，在邻域内寻找更优解

目标是找到一个局部最优解，它在其邻域内没有更好的解，但未必是全局最优解

- 爬山算法
  - 从初始解出发，评估邻域中的所有可能解，选择最优解作为新的当前解，重复直到没有更优解
- 遗传算法
  1. 初始化
     - 随机生成一群个体（初始种群，代表一组可能的解）
  2. 适应度分配
     - 计算每个个体的“适应度”（分数），分数越高代表解越好
  3. 选择 (Selection)
     - 挑选适应度高的个体作为“父母”，淘汰差的
  4. 交叉 (Crossover)
     - 父母交换基因（特征），产生新一代子代
  5. 变异 (Mutation)
     - 对子代进行小概率的随机改变（防止陷入局部最优，增加多样性）
  6. 迭代
     - 重复上述过程，直到满足停止条件

#### 对抗搜索

主要用于解决博弈问题，需要考虑对手的行动

在两个对立的玩家之间交替进行搜索，目的是通过预测对手的行动来找到自己的最优策略

- 极大极小算法

  - 假设有两个玩家，Max（我方）和 Min（对手）

    - Max 想要让最终得分（收益）最大化
    - Min 想要让 Max 的得分最小化（即对手会采取对我们最不利的行动）。

  - 工作流程：

    1. 构建博弈树： 向下推演几步，直到达到某个深度或游戏结束。
    2. 评估： 给底部的节点（结局）打分。
    3. 倒推（回溯）：

       - 在 Min 层（对手行动），选择子节点中分数最小的那个传上去（因为对手很聪明，会选对我们最不利的）
       - 在 Max 层（我方行动），选择子节点中分数最大的那个传上去（我们要选对自己最有利的）

  - 结论： 通过这种交替的“找最大”和“找最小”，找到当前的最优策略。

- Alpha-Beta 剪枝
  - 在搜索过程中维护两个值：
    - $\alpha$： 我方（Max）目前为止找到的最好（最高）的保底收益。
    - $\beta$： 对手（Min）目前为止能限制我的最坏（最低）的上限收益。
  - 剪枝逻辑：
    - 如果在搜索某个分支时，发现对手可以让局面变得比我已知的最好结果 $\alpha$ 还要差（或者相等），那么这个分支就没必要继续搜下去了。
    - 因为理性的我绝不会选这条路，所以这条路剩下的细节无关紧要，直接“剪掉”

## 机器学习

### 基本要素

#### 机器学习的定义

- 机器学习是人工智能的一个子集
- 使用==统计方法==使机器挖掘数据中的规律，进而实现特定的目标

#### 机器学习的关键要素

- 数据 (Data) —— 经验
  - 机器学习的“原材料”，也就是机器用来学习的经验
- 模型 (Model) —— 假设
  - 对数据规律做出的假设，即一个待定系数的数学函数
- 损失函数 (Loss Function) —— 目标
  - 用来衡量模型预测好坏的标准（性能准则）。它计算预测值与真实值之间的误差
- 参数优化 (Parameter Optimization) —— 改进
  - 通过算法不断调整模型参数，使损失函数达到最小值的过程。这是真正的“学习”动作

机器学习就是利用算法（优化），从数据中学习，不断调整模型的参数，以最小化损失函数的过程

### 相关概念

#### 数据预处理

- 清洗
  - 处理缺失值、异常值、重复数据等，确保数据质量
- 标准化
  - 抹除不同变量的幅值差异，将它们缩放到一个相似的范围内
  - 可以辅助提升机器学习模型的训练效率
  - 标准化不会改变数据的信息量，只是改变了数据的数值表达方式
- 编码
  - 将非数值数据转换为计算机可理解的数值向量

#### 特征工程

- 将原始数据转换为特征的过程。简单来说，就是从复杂的数据中“提炼”出对模型预测最有用的关键信息，让计算机更容易理解
- 优点：
  - 简化数据
    - 降低了数据的维度和复杂度。
  - 提升效率
    - 可以辅助提升机器学习模型的训练效率
  - 可解释性
    - 提取的特征（如“峰值”）通常具有明确的物理意义，让人更容易理解模型是根据什么做判断的。
- 缺点：
  - 信息损失
    - 提炼过程中，原始数据中的某些细节可能会丢失
    - 依赖专家知识
      - 在传统机器学习中，这通常需要人工干预和专业领域的知识

#### 泛化

- 泛化是指模型在未见过的、新的数据上的表现能力
- 如果模型在训练集上得分很高，但在测试集（未见过的数据）上得分很差，我们就说这个模型的泛化能力差，这通常意味着发生了过拟合

#### 过拟合与欠拟合

![三种拟合情况](https://raw.githubusercontent.com/dcldyhb/Freshman-Notes-Image-Host/main/202601160323158.png)

- 欠拟合
  - 模型过于简单（复杂度低）
  - 还没学会数据的规律。它在训练集和测试集上的表现都很差。它无法捕捉数据的真实分布
- 过拟合
  - 模型过于复杂（复杂度高）
  - 在训练集上表现完美，但在测试集（未见过的数据）上表现很差，拟合了数据的噪声而不是真是的数据分布

#### 偏差和方差

- 偏差
  - 衡量模型的准确性
  - 预测值与真实值之间的系统性偏差
- 方差
  - 衡量模型的稳定性
  - 样本之间的离散程度

| 模型状态   | 复杂度 | 偏差 (Bias)   | 方差 (Variance) | 理解                                                                                             |
| :--------- | :----- | :------------ | :-------------- | :----------------------------------------------------------------------------------------------- |
| **欠拟合** | 低     | **高 (High)** | 低 (Low)        | 模型太“僵化”，无论数据怎么变，它都预测不准（偏差大），但也因为僵化，它很稳定（方差小）。         |
| **过拟合** | 高     | 低 (Low)      | **高 (High)**   | 模型太“敏感”，它能完美拟合训练数据（偏差小），但换一批数据预测结果就剧烈变化（方差大，不稳定）。 |

- 偏差-方差困境：欠拟合和过拟合都是需要避免的，需要在偏差和方差之间找到一个平衡点，找到适合的模型复杂度

#### 防止过拟合

- 选择合适复杂度的模型
- 增加训练数据
- 正则化（惩罚模型复杂度）
- 留出法和交叉验证（使用未见过的数据确保泛化）
- 提前停止训练
- 利用先验知识（如：贝叶斯先验）

##### 划分数据集

- 训练集
  - 用于训练模型，调整模型参数
  - 一般最多
- 验证集
  - 用于调参和选择模型
  - 帮助检测过拟合
- 测试集
  - 用于评估模型的最终性能
  - 模拟模型在真实环境中的表现

##### 交叉验证

在数据集很小时，使用交叉验证来更有效地利用数据

- K 折交叉验证
  - 将数据集划分为 K 个子集
  - 每次用 K-1 个子集训练，剩下的一个子集测试
  - 重复 K 次，取平均性能作为最终评估

![K-折交叉验证](https://raw.githubusercontent.com/dcldyhb/Freshman-Notes-Image-Host/main/202601160355020.png)

##### 提前停止训练

在验证损失最小时停止训练，避免模型对训练数据的过拟合

##### 正则化

添加一个对模型参数的惩罚项，防止模型对训练数据过拟合

### 优化算法

#### 梯度下降

- 通过计算梯度，不断更新参数，让模型误差越来越小，直到找到最优模型的过程

#### 启发式算法

- 一类基于经验和直觉的优化方法
- 不保证找到全局最优解（最好的那个答案），但致力于在可接受的计算成本（时间）内，找到一个“足够好”的解

##### 贪心算法

- 每一步选择中都采取在当前状态下最优的选择
- 根据每次迭代的信息做出判断，而不考虑更广泛的问题
- 无法为每个问题提供最佳答案，容易陷入局部最优

##### 模拟退火算法

- 高温阶段
  - 分子动能大，对应寻优过程中探索幅度大，以便在早期能越过局部最优解继续探索
- 降温过程
  - 随着时间（迭代次数）增长，逐渐向最优靠拢，但同时逐渐冷却，探索幅度减小
- 冷却
  - 不再探索，获得最终结果

##### 蚁群算法

- 由自然界中蚂蚁觅食的行为而启发的

1. $N$ 只蚂蚁开始自由探索
2. 碰到还没走过的路口，就随机挑选一条路走。同时，释放与路径长度有关的信息素。路径越短信息素浓度越高
3. 蚂蚁有较大概率选择高信息素，越来越集中在信息素高的路径，长路径信息素逐渐挥发（正反馈）。
4. 最终蚂蚁集中在最优路径上。

##### 粒子群算法

- 模仿鸟群觅食
- 群体中的个体通过信息共享，根据自己和同伴的经验，调整飞行方向，协作找到最优的目的地

##### 遗传算法

同上

## 传统机器学习模型

### 线性模型

- 回归问题
  - 预测一个连续的数值
- 分类问题
  - 预测一个离散的类别

### 决策树

- 通过树状结构进行决策
- 自上而下，选择最能区分数据的属性进行分裂，递归地将数据划分为越来越纯的子集，直到满足停止条件为止
  1. 构建包含整个数据集的根节点
  2. 选择最符合分类标准的属性
  3. 根据所选属性将当前节点的样本分成子集
  4. 为每个子集创建一个子节点
  5. 递归地重复步骤 2~4 ，直到满足停止准则

![决策树](https://raw.githubusercontent.com/dcldyhb/Freshman-Notes-Image-Host/main/202601160424915.png)

- 决策树可以用于回归
  - 将平均值作为预测值
- 天然具有过拟合特质
  - 通过剪枝能够减少过拟合

### 贝叶斯

![贝叶斯决策](https://raw.githubusercontent.com/dcldyhb/Freshman-Notes-Image-Host/main/202601160751917.png)

- 贝叶斯神经网络最大的特点是其神经元的权重是一个概率分布，而不是一个确定的值

### 支持向量机

- 最大间隔化
  - 通过最大化支持向量到决策边界的距离，来提高模型的泛化能力
- 线性不可分
  - 在二维平面上，有些数据是没法用一条直线分开的。这种情况就叫线性不可分
  - 把数据投影（映射）到高维空间
  - 核函数
    - 核函数是一种数学工具，可以在不显式计算高维空间坐标的情况下，计算数据点在高维空间中的内积
    - 常见核函数有线性核、多项式核、径向基函数（RBF）核等
- 多分类
  - 一对多
  - 一对一
- 用于回归
  - 寻找一个最合适的管道，使样本点尽可能多的在管道内。中间的超平面被当做预测值

### 最近邻算法

#### K-最近邻算法（KNN）

- 基于实例的学习，也称为懒惰学习
  - 仅仅存储训练数据而不是学习一个模型
  - 每当有新的数据需要分类时，就从训练数据中找到其 $K$ 个最近邻
  - 储存所有可用案例，根据相似度度量对新数据或案例进行分类
  - 一般比较耗时

1. 确定参数 $𝐾$
2. 从需要分类的测试数据中选择一个样本，并计算它与
3. 所有训练样本之间的距离
4. 对获得的距离进行排序，并选取最近的 $𝐾$ 个数据样本
5. 根据这 $𝐾$ 个邻居的多数投票，将测试样本分配到相应的类别

## 神经网络与深度学习

### 多层感知机

- 结构组成
  - 是一种**前馈神经网络**
  - **输入层 (Input Layer)：** 接收原始数据（如图像像素的向量）
  - **隐藏层 (Hidden Layers)：** 位于中间，负责提取特征。可以有一层或多层，层数越多，网络越“深”
  - **输出层 (Output Layer)：** 输出最终的预测结果（如分类标签）
- 核心特征：全连接 (Fully Connected)
  - **连接方式：** 上一层的**每一个**神经元都与下一层的**每一个**神经元相连
  - **工作原理：** 信号通过加权求和及非线性激活函数，一层层向后传递
- 优点
  - 强大的拟合能力，能处理复杂的非线性关系
  - 通用性强，适用于多种任务（分类、回归等）
- 缺点
  - 梯度消失、梯度爆炸等
  - 参数量庞大，训练耗时长

### 卷积神经网络（CNN）

- 在图像上进行空间滑动，进行特征提取
- 在滑动过程中，神经元的权重共享
- 计算参数
  - 核大小(Kernel Size)
    - 卷积核的尺寸，决定了每次卷积覆盖的区域大小
  - 步长(Stride)
    - 决定了卷积核在输入数据上移动的步幅
  - 填充(Padding)
    - 对原始图片的边界添加额外的零填充的尺寸
  - 输入通道数(Input Channel)
    - 取决于输入数据，可被视为输入特征图的深度
  - 输出通道数(Output Channel)
    - 由卷积核的数量决定，每个通道是输入数据的特征映射
- 计算参数量公式

  $$
  \text{Output} = \left\lfloor \frac{\text{Input} + 2 \times \text{Padding} - \text{Kernel}}{\text{Stride}} \right\rfloor + 1
  $$

  +1 是偏置项

![卷积操作](https://raw.githubusercontent.com/dcldyhb/Freshman-Notes-Image-Host/main/202601160856979.png)

- 卷积的维数是指卷积核的移动方向的维度，而不是数据或卷积核的维数
- 池化（Pooling）:池化的核只负责框定某次池化计算需要的图片的范围，不需要学习权重。
  - 最大池化
    - 在池化窗口（Kernel）覆盖的区域内，选出数值最大的那一个，丢弃其他的
- 经典 CNN 架构
  - LeNet
  - AlexNet
  - VGG
  - Inception
  - ResNet
    - 跨层链接
    - 从数据直接引一个输入过来，这样的话可以让这个网络训练更稳定
  - DenseNet

### 循环神经网络（RNN）

- 用于处理==序列数据==（如文本、时间序列等）
- 面对长期依赖性问题
- 长短期记忆网络
  - 将长期记忆和短期记忆区分开来

### Attention 和 Transformer

- ==是现有大模型体系的核心模块==
- 掌握其相比 RNN 的最大优势：解决了信息瓶颈问题和长序列依赖问题

### 位置编码

- 解决 Attention 丢失单词位置信息的问题

### 强化学习（RL）

- 强调如何基于环境而行动，以取得==最大化的长期利益==
- ==核心要素==
  - 智能体(Agent)
    - 一个能够观察环境、采取行动并根据反馈做出决策的实体
  - 环境(Environment)
    - 智能体所处的外部世界，智能体通过与环境的交互来获取信息和奖励。
  - 动作(Action)
    - 代理在给定的状态下可以选择执行的决策或操作。
  - 动作空间(action space)
    - 一个集合，包含了智能体在每个状态下可以采取的所有可能动作
  - 奖励(reward)
    - 一个数值，用于衡量智能体采取某个行动后获得的即时收益
  - 状态(State)
    - 代表了环境在某一时间点的一个具体状况或配置
  - 策略(Policy)
    - 从状态映射到动作的函数
- 理论依据
  - 马尔可夫过程/链
- 实现方式
  - 价值学习
    - 通过学习价值函数，评估状态/动作的好坏，进而间接推导出策略
  - 策略学习
    - 直接学习策略函数，输出应该采取的行动

能源领域落地较少的原因

- 强化学习是做决策，还会犯错，能源领域对安全性要求很高，不能随便犯错，成本过高

### 生成对抗网络（GAN）

- 学会的是数据的概率分布
- 两个神经网络
  - 生成器（Generator）
    - 生成数据，目的是为了骗过判别器
    - 输入是随机噪声，输出是生成的数据
  - 判别器（Discriminator）
    - 判断数据是真实的还是生成的，目的是找出生成器做的“假数据”
  - 生成器和判别器互相进行对抗
- 经典 GAN 局限性
  - 无法控制生成结果（不可控）
- 条件 GAN
  - 通过加入条件实现定向生成

## 大数据与云计算

### 基本概念

#### 大数据的 5V 特征

- 数据量（Volume）
  - 规模巨大
- 多样性（Variety）
  - 数据类型多样
- 价值（Value）
  - 潜在价值高
- 速度（Velocity）
  - 生成和处理速度快
- 真实性（Veracity）
  - 数据质量参差不齐
  - 不一致性和不确定性

#### 大数据的类型

- 结构化数据
  - 具有明确结构的标准化格式
  - 使用预定义的数据模型，填充标签、数字和值
  - 如：Excel 表格、电子表格、数据库
- 半结构化数据
  - 缺乏精确的功能结构但具有某些结构特性
  - 主要是非结构化的，但有内部标签和标记来帮助分类
  - 如：NoSQL 数据库、JSON 文件、XML 文件
- 非结构化数据
  - 没有预先确定的形状或结构
  - 难以使用传统数据分析工具进行分析
  - 如：文本文件、电子邮件、音视频

### 大数据处理方法

![独热编码](https://raw.githubusercontent.com/dcldyhb/Freshman-Notes-Image-Host/main/202601161021843.png)

- 独热编码
  - 避免数值大小关系，赋予均等地位
- 元数据
  - 关于数据的数据，它描述了数据的结构、内容、上下文等
  - 元数据提供了关于数据的信息，使数据更容易被理解

### 云计算基本概念

- 通过租用的方式，把自己的数据存储和计算在供应商提供远端的服务器上进行
- 把计算服务与数据存储作为一种商品进行售卖或者租赁，购买后可以在云端提供服务
- 云计算的类型
  - 公有云
    - 由第三方云服务提供商拥有和运营，向公众开放
    - 成本更低
      - 无需购买硬件，仅对使用的服务付费
    - 无需维护
      - 维护由服务提供商提供
    - 近乎无限制的缩放性
      - 提供按需资源
    - 高可靠性
      - 具备众多服务器，确保免受故障影响
  - 私有云
    - 由单一组织拥有和运营，专门为该组织提供服务
    - 灵活性更高
      - 可自定义云环境
    - 安全性更高
      - 资源不与其他组织共享
    - 缩放性更高
      - 具有一定的的缩放性和效率
  - 混合云
    - 结合了公有云和私有云的优点，允许数据和应用在两者之间进行迁移
    - 控制性
      - 组织可针对敏感资产维持私有基础结构
    - 灵活性
      - 需要时可利用公有云中的其他资源
    - 成本效益
      - 具备扩展至公有云的能力
    - 容易轻松
      - 无需费时费力即可转换至云
- 公有云的服务形式
  - 基础设施即服务 (IaaS)
    - 提供虚拟化的计算资源（如服务器、存储、网络）
    - 用户负责操作系统、应用程序等的管理
    - Amazon EC2、Microsoft Azure
  - 平台即服务 (PaaS)
    - 提供开发和部署应用程序的平台
    - 用户只需关注应用程序的开发和管理，基础设施由服务提供商管理
    - Google App Engine、Microsoft Azure App Services
  - 软件即服务 (SaaS)
    - 提供基于云的应用程序，用户通过浏览器或客户端访问
    - 用户无需管理底层基础设施和平台
    - Google Workspace、Microsoft 365

## 计算机视觉（CV）

### 主要任务

- 分类
- 语义分割
- 目标检测
- 目标描述
- 图像生成

### 语义分割

- ==是一种像素级别的分类方案==

### Vision Transformer (ViT)

Transformer 架构可用于处理图像

### 图像生成

- 去噪扩散模型
  - 正向扩散过程
  - 反向去噪过程（生成图片）

## 自然语言处理（NLP）

- 属于==序列问题==
- 很多问题用于解决长距离依赖
- 两个阶段
  - 预训练
    - 属于自监督学习，下文是上文的标签
  - 微调
    - 训练好的大模型针对具体任务改良
- 两种模型
  - Bert
    - 双向模型
  - GPT
    - 单向模型

### 多模态问题

能够协同处理不同类型数据（图像、文本、声音）

## 应用

### AI 在能源领域具有的优势

1. 复杂规律学习
   - 尤其是深度学习技术，可以从大量数据中学习到复杂、高维、非线性的内在规律（如预测问题）。这些规律的复杂度往往超出人类的能力范围。
2. 动态适应能力
   - 知识来源是数据。数据可以不断地进行更新迭代，使得机器学习模型可以不断地基于最新的数据实现自我更新。这在建模领域尤其有价值
3. 提升工作效率
   - 在部分业务环节，通过机器学习技术为人的决策提供支撑（例如故障恢复）或者替代人类决策（例如电压调节），实现效率的提升。模型的前向计算速度快

### 机器学习在能源领域面临哪些挑战？

1. 数据敏感性问题
   - 能源部门处理大量敏感数据，确保这些数据的安全至关重要。必须保护人工智能系统免受网络威胁和破坏。遵守 GDPR 等数据隐私法规会增加额外的复杂性。
2. 鲁棒性与可解释性
   - 机器学习得到的结论目前难以解释，且难以完全避免错误结果的产生，制约了其应用，导致其多数应用在决策支持领域，难以在关键环节直接进行决策。
3. 缺乏专业人员
   - 缺乏既了解能源领域又了解人工智能技术的训练有素的人工智能专业人员。懂机器学习的不懂能源行业，懂能源行业的不懂机器学习。

### 机器学习在能源领域的未来在哪里？

1. 规范行业数据
   - 以政府等具有公信力的部门，建立行业数据库以及配套的保密、交易机制，为能源领域机器学习的开展提供充足的数据支持。
2. 数据-机理融合
   - 通过研究将能源领域长期积累的物理模型与机器学习模型的融合（例如物理约束嵌入损失函数），实现两类模型的优势互补，解决机器学习的可解释性、鲁棒性问题。
3. 培养交叉人才
   - 培养既能深入了解行业专业知识，又具有扎实的机器学习应用能力的学科交叉人才，真正发挥机器学习这类强大工具的价值，解决能源领域的工程问题
